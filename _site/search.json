[
  {
    "objectID": "Contact.html",
    "href": "Contact.html",
    "title": "Stats & R",
    "section": "",
    "text": "Contact\n\n\n\n\n\nThanks in advance for contacting me.\nIn order for me to answer you as soon as possible, here are the best communication methods:\n\nDue to the increasing number of questions received, responding to each of them by email has become unmanageable and unproductive. Therefore, if you have a question, I invite you to add it as a comment at the end of the corresponding article. This way, other readers can benefit from the discussion and that saves me from answering the same thing several times. Questions by email will not be answered (I will redirect you to the comments).\nFor mistakes, bugs or inconsistencies—and that can happen, we are all human after all—you can inform me about them by raising an issue on GitHub. If you are familiar with GitHub, you can even edit the corresponding file and propose a pull request so I can quickly include your changes.\nFor all other requests, please use the contact form below\n\nIf you need to send a file, first fill in the contact form to which I will reply and from there you will be able to send me your file (this is to limit spam)\n\n\nFor a quick answer to your question, make sure to first check the FAQ and comments from other readers. Your question may have already been answered."
  },
  {
    "objectID": "posts/Dynamic Graph/Graph.html",
    "href": "posts/Dynamic Graph/Graph.html",
    "title": "Dynamic Graph",
    "section": "",
    "text": "Here is an example shows how to use gganmate package to make a dynamic graph."
  },
  {
    "objectID": "posts/Dynamic Graph/Graph.html#data-from-mtcars",
    "href": "posts/Dynamic Graph/Graph.html#data-from-mtcars",
    "title": "Dynamic Graph",
    "section": "Data from mtcars",
    "text": "Data from mtcars\n\nlibrary(ggplot2)\nlibrary(gganimate)\n\nggplot(mtcars, aes(factor(cyl), mpg)) + \n  geom_boxplot() + \n  # Here comes the gganimate code\n  transition_states(\n    gear,\n    transition_length = 2,\n    state_length = 1\n  ) +\n  enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')"
  },
  {
    "objectID": "posts/Dynamic Graph/Graph.html#data-from-gapminder",
    "href": "posts/Dynamic Graph/Graph.html#data-from-gapminder",
    "title": "Dynamic Graph",
    "section": "Data from gapminder",
    "text": "Data from gapminder\n\nlibrary(gapminder)\n\nggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, colour = country)) +\n  geom_point(alpha = 0.7, show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  scale_x_log10() +\n  facet_wrap(~continent) +\n  # Here comes the gganimate specific bits\n  labs(title = 'Year: {frame_time}', x = 'GDP per capita', y = 'life expectancy') +\n  transition_time(year) +\n  ease_aes('linear')"
  },
  {
    "objectID": "posts/welcome/welcome.html",
    "href": "posts/welcome/welcome.html",
    "title": "Welcome",
    "section": "",
    "text": "This is my first blog on Stats & R. Hope you will find some joys here !!!"
  },
  {
    "objectID": "posts/Christmas Tree/christmas_tree.html",
    "href": "posts/Christmas Tree/christmas_tree.html",
    "title": "Christmas Tree",
    "section": "",
    "text": "Let us use ggplot2 to make a Christmas Tree.\n\nCode\n\nrm(list = ls())\nlibrary(ggplot2)\n\n# create data\nx <- c(8,7,6,7,6,5,6,5,4,5,4,3,4,3,2,3,2,1,0.5,0.1)\n\ndat1 <- data.frame(x1 = 1:length(x), x2 = x)\ndat2 <- data.frame(x1 = 1:length(x), x2 = -x)\ndat1$xvar <- dat2$xvar <- NA\ndat1$yvar <- dat2$yvar <- NA\ndat1$siz <- dat2$siz <- NA\ndat1$col <- dat2$col <- NA\n\n# set threshold for christmas balls\ndec_threshold = -0.5\n\n# create random places, sizes and colors for christmas balls\nset.seed(2512)\nfor (row in 1:nrow(dat1)){\n\nif (rnorm(1) > dec_threshold){\n\ndat1$xvar[row] <- row\ndat1$yvar[row] <- sample(1:dat1$x2[row]-1,1)\ndat1$siz[row] <- runif(1,0.5,1.5)\ndat1$col[row] <- sample(1:5, 1)\n}\n\nif (rnorm(1) > dec_threshold){\n\ndat2$xvar[row] <- row\ndat2$yvar[row] <- sample(1:dat2$x2[row],1)\ndat2$siz[row] <- runif(1,0.5,1.5)\ndat2$col[row] <- sample(1:5, 1)\n}\n}\n\n# plot the christmas tree\nggplot() +\ngeom_bar(data = dat1, aes(x=x1, y=x2),stat = \"identity\", fill = '#31a354') +\ngeom_bar(data = dat2, aes(x=x1, y=x2),stat = \"identity\", fill = '#31a354') +\ngeom_point(data = dat1,aes(x = xvar, y = yvar, size = siz, colour = as.factor(col)) ) +\ngeom_point(data = dat2,aes(x = xvar, y = yvar, size = siz, colour = as.factor(col)) ) +\ncoord_flip() + theme_minimal()+ theme(legend.position=\"none\",\naxis.title.x=element_blank(),\naxis.text.x=element_blank(),\naxis.ticks.x=element_blank(),\naxis.title.y=element_blank(),\naxis.text.y=element_blank(),\naxis.ticks.y=element_blank()) +\nggtitle('Merry Christmas') +\ntheme(plot.title = element_text(color = \"red\", hjust = 0.5))"
  },
  {
    "objectID": "Who.html",
    "href": "Who.html",
    "title": "Who am I ?",
    "section": "",
    "text": "Hi, I’ m Ning and I’m a potential data analyst.\nMy current focus is statistics, R-based data analysis and corporate-related political risk issues.\nMy background is in Accountancy. If you have any doubts about corporate auditing issues, maybe i can help you.\nI really hate graphical interface data analysis software, because it always gives me extra results that I didn’t ask for.\nMy current short-term goal is to publish my first peer-reviewed paper and start my PhD by 2023.\nIn my free time, I like listening to music, hiking and reading. If you also like these activities, please feel free to contact me."
  },
  {
    "objectID": "Who.html#education",
    "href": "Who.html#education",
    "title": "Who am I ?",
    "section": "Education",
    "text": "Education\nMassey University, Auckland | Master in Accountancy | February 2019 - June 2020"
  },
  {
    "objectID": "Who.html#experience",
    "href": "Who.html#experience",
    "title": "Who am I ?",
    "section": "Experience",
    "text": "Experience\nLife Maid Easy | Head Marketing | December 2020 - present"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stats & R",
    "section": "",
    "text": "Welcome to Stats & R !!!\nThis is a personal website with Statistics and the open source language R as the main content, aiming to facilitate academic exchanges and better promote myself.\nIf you are new to this site, you will notice that the site is made up of four sections: Blog, About, Works and Contact.\nHere are some useful suggestions that can help you use this site smoothly.\n\nThe Blog page, some interesting ideas(how to use R to make a 3D Christmas tree, etc.) in daily life will be presented.\nThe About page, a good place where you can quickly get to know who i am and what i am doing now.\nThe Works page, Independent projects, and R language study notes will be showing here.\nIf you have any questions, please go to the contact page to get in touch directly.\n\nThanks again for coming to my personal website, i hope it can be helpful for your concern."
  },
  {
    "objectID": "Datavisualization.html",
    "href": "Datavisualization.html",
    "title": "Data Science: Data Visualization",
    "section": "",
    "text": "Data visualization is the graphical representation of information and data. By using visual elements like charts, graphs, and maps, data visualization tools provide an accessible way to see and understand trends, outliers, and patterns in data. Additionally, it provides an excellent way for employees or business owners to present data to non-technical audiences without confusion.\nIn the world of Big Data, data visualization tools and technologies are essential to analyze massive amounts of information and make data-driven decisions."
  },
  {
    "objectID": "Datavisualization.html#overview",
    "href": "Datavisualization.html#overview",
    "title": "Data Science: Data Visualization",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nAfter completing this section, we will:\n\nunderstand the importance of data visualization for communicating data-driven findings.\nbe able to use distributions to summarize data.\nbe able to use the average and the standard deviation to understand the normal distribution\nbe able to access how well a normal distribution fit the data using a quantile-quantile plot.\nbe able to interpret data from a box plot"
  },
  {
    "objectID": "Datavisualization.html#introduction-to-data-visualization",
    "href": "Datavisualization.html#introduction-to-data-visualization",
    "title": "Data Science: Data Visualization",
    "section": "2.2 Introduction to Data Visualization",
    "text": "2.2 Introduction to Data Visualization\n\n2.2.1 Key Point:\n\nPlots of data easily communicate information that is difficult to extract from table of raw values.\nData visualization is a key component of exploratory data analysis (EDA), in which the properties of data are explored through visualization and summarization techniques.\nData visualization can help discover biases, systematic errors, mistakes and other unexpected problems in data before those data are incorporated into potentially flawed analysis.\nBasics of data visualization and EDA will be covered in R by using the ggplot2 package and motivating examples from world health, economics and infections disease.\n\n\n\n2.2.2 Code:\n\n\nshow the code\nlibrary(dslabs)\ndata(murders)\nhead(murders)\n\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65"
  },
  {
    "objectID": "Datavisualization.html#introduction-to-distributions",
    "href": "Datavisualization.html#introduction-to-distributions",
    "title": "Data Science: Data Visualization",
    "section": "2.3 Introduction to Distributions",
    "text": "2.3 Introduction to Distributions\n\n2.3.1 Key Points:\n(Variance/Deviation Var)方差: 方差越大，数据的波动越大；方差越小，数据的波动就越小。\n(Standard Deviation)标准差: 方差开根号。\n\nThe most basic statistical summary of a list of object is its distribution.\nWe will learn ways to visualize and analyze distributions in the upcoming videos.\nIn some cases, data can be summarized by two-number summary: the average and standard deviation.I will learn to use data visualization to determine when that is appropriate.\n\n\n\n2.3.2 Data Types\nIn R, there are 6 basic data types:\n\nlogical\nnumeric\ninteger\ncomplex\ncharacter\nraw\n\n\n\n\n\n\n\nImportant\n\n\n\nCategorical data are variables that are defined by a small number of groups.\n\nOrdinal categorical data have an inherent order to the categories (mild/medium/hot, for example).\nNon-ordinal categorical data have no order to the categories.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNumerical data take a variety of numeric values.\n\nContinuous variables can take any value.\nDiscrete variables are limited to sets of specific values.\n\n\n\n\n\n\n\nflowchart LR\n  A[Main variable types] --> B{Catrgorical}\n  A[Main variable types] --> C{Numeric}\n  B{Catrgorical} --> D[ordinal]\n  B{Catrgorical} --> E[non-ordinal]\n  C{Numeric} --> F[continuous]\n  C{Numeric} --> G[discrete]\n\n\n\n\n\n\n\n\n\n\n2.3.3 Exercise\n\n\nshow the code\n# extract the variable names from a dataset\nnames(x)\n# explore how many unique values are used in dataset\nunique(x)\n# determine how many variable were reported\nlength(x)\n# determine how many unique variable were reported\nlength(unique(x))\n# to compute the frequencies of each unique value\ntable(x)"
  },
  {
    "objectID": "Datavisualization.html#describe-heights-to-et",
    "href": "Datavisualization.html#describe-heights-to-et",
    "title": "Data Science: Data Visualization",
    "section": "2.4 Describe Heights to ET",
    "text": "2.4 Describe Heights to ET\n\n2.4.1 key point:\n\nA distribution is a function or description that shows the possible values of a variable and how often those values occur.\nFor categorical variables, the distribution describes the proportions of each category.\nA frequency table is the simplest way to show a categorical distribution. Use prop.table() to convert a table of counts to a frequency table. Barplots display the distribution of categorical variables and are a way to visualize the information in frequency tables.\nFor continuous numerical data, reporting the frequency of each unique entry is not an effective summary as many or most values are unique. Instead, a distribution function is required.\nThe cumulative distribution function (CDF) is a function that reports the proportion of data below a value a for all values of a :F(a)=Pr(x≤a).\nThe proportion of observations between any two values a and b can be computed from the CDF as F(b)-F(a).\nA histogram divides data into non-overlapping bins of the same size and plots the counts of number of values that fall in that interval.\n\n\n\n2.4.2 Code:\nR 语言学习 - table() 结果提取.\n\n\nshow the code\n# load the dataset\nlibrary(dslabs)\ndata(heights)\n# make a table of category proportions\nprop.table(table(heights$sex))"
  },
  {
    "objectID": "Datavisualization.html#cumulative-distribution-function",
    "href": "Datavisualization.html#cumulative-distribution-function",
    "title": "Data Science: Data Visualization",
    "section": "2.5 Cumulative Distribution Function",
    "text": "2.5 Cumulative Distribution Function\nEvery continuous distribution has cumulative distribution function (CDF). The CDF defines the proportion of the data below a given value for all values of a :\n\n\n\nCumulative Distribution Function (CDF)\n\n\nAs defined above, this plot of the CDF for male heights has height value a on the x-axis and the proportion of student with heights of that value or lower(F(a)) on the y-axis.\nThe CDF is essential for calculating probabilities related to continuous data. In a continuous dataset, the probability of a specific exact value is not informative because most entries are unique. For example, in the student heights data, only one individual reported a height of 68.8976377952726 inches, but many students rounded similar heights to 69 inches. If we computed exact value probabilities, we would find that being exactly 69 inches is much more likely than being a non-integer exact height, which does not match our understanding that height is continuous. We can instead use the CDF to obtain a useful summary, such as the probability that a student is between 68.5 and 69.5 inches.\nFor datasets that are not normal, the CDF can be calculated manually by defining a function to compute the probability above. This function can then be applied to a range of values across the range of the dataset to calculate a CDF. Given a datasetmy_data, the CDF can be calculated and plotted like this:\nR语言中的[apply()]，[lapply()]，[sapply()]，tapply()函数以及示例\n\n2.5.1 Code for CDF:\n\n\nshow the code\n# Cumulative Distribution Function \na <- seq(min(x), max(x), length) # define range of the values\ncdf_function <- function(x) {\n    mean(my_data <= x)\n}\ncdf_values <- sapply(a, cdf_function)\nplot(a, cdf_values)\n\n\n\n\n2.5.2 Code for student height:\n\n\nshow the code\n# example for student heights\na <- seq(min(heights$height), max(heights$height), length = 100)\ncdf_function <- function(x){\n  mean(heights$height <= x)\n}\ncdf_value <- sapply(a, cdf_function)\nplot(a, cdf_value)\n\n\n\n\n\nThe CDF defines that proportion of data below a cut-off a. To define the proportion of values above a, we compute: 1-F(a)\nTo define the proportion of values between a and b, we compute: F(b)-F(a)\nNote that the CDF can help compute probabilities. The probability of observing a randomly chosen value between a and b is equal to the proportion of values between a and b, which we compute with the CDF."
  },
  {
    "objectID": "Datavisualization.html#smooth-density-plots",
    "href": "Datavisualization.html#smooth-density-plots",
    "title": "Data Science: Data Visualization",
    "section": "2.6 Smooth Density Plots",
    "text": "2.6 Smooth Density Plots\n\n2.6.1 Key Point:\n\n\n\n\n\n\nA further note on histograms\n\n\n\nThe choice of binwidth has a determinative effect on sharp. There is no “correct” choice for binwidth, and you can sometimes gain insights into the data by experimenting with binwidths.\n\n\n\nSmooth density plots can be thought of as histograms where the binwidth is extremely or infinitely small. The smoothing function makes estimates of the true continuous trend of the data given the available sample of data points.\nThe degree of smoothness can be controlled by an argument in the plotting function.\nWhile the histogram is an assumption-free summary, the smooth density plot is shaped by assumptions and choices you make as a data analyst.\nThe y-axis is scaled so that the area under the density curve sums to 1. This means that interpreting value on the y-axis is not straightforward. To determine the proportion of data in between two values, compute the area under the smooth density curve in the region between those values.\nAn advantage of smooth densities over histograms is that densities are easier to compare visually."
  },
  {
    "objectID": "Datavisualization.html#normal-distribution",
    "href": "Datavisualization.html#normal-distribution",
    "title": "Data Science: Data Visualization",
    "section": "2.7 Normal Distribution",
    "text": "2.7 Normal Distribution\n\n2.7.1 Key Points:\n\nThe normal distribution:\n\nis centered around one value, the mean\nis symmetric(对称) around the mean.\nis defined completely by its mean(\\mu) and standard deviation(\\sigma)\nAlways has the same proportion of observations within a given distance of the mean (for example, 95% with 2\\sigma)\n\nThe standard deviation is the average distance between a value and the mean value.\nCalculate the mean using the mean() function.\nCalculate the standard deviation using the sd() function or manually.\nStandard units describe how many standard deviations a value is away from the mean. The z-score, or number of standard deviation an observation is away from the mean \\mu:\n\n  z = (x-\\mu)/\\sigma\n  \nComputer standard units with the scale() function.\nImportant: to calculate the proportion of value that meet a certain condition, use the mean function on a logical vector. Because TRUE is converted to 1 and FALSE is converted to 0, taking the mean of this vector yields the proportion of TURE."
  },
  {
    "objectID": "Datavisualization.html#equation-for-the-normal-distribution",
    "href": "Datavisualization.html#equation-for-the-normal-distribution",
    "title": "Data Science: Data Visualization",
    "section": "2.8 Equation for the normal distribution",
    "text": "2.8 Equation for the normal distribution\nThe normal distribution is mathematically defined by the following formula for any mean \\mu and standard deviation \\sigma:\n\nPr(a < x < b) = \\int_{a}^b\\frac{1}{\\sqrt{2\\pi\\mu}}{e}^{-\\frac{1}{2}(\\frac{x-\\mu^2}{\\sigma})}dx\n\nWhen standard unites z=0, the normal distribution is at a maximum, the mean \\mu. The function is defined to be symmetric around z=0.\nThe normal distribution of z-score is called the standard normal distribution and is defined by \\mu=0 and \\sigma=1.\nZ-score are useful to quickly evalute whether an observation is average or extreme. Z-scores near 0 are average. Z-score above 2 or below -2 are significantly above or blew the mean, and z-scores above 3 or below -3 are extrmely rate.\n\n2.8.1 Code:\n\n\nshow the code\n# define x as vector of male heights\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(heights)\nindex <- heights$sex==\"Male\"\nx <- heights$height[index]\n\n# calculate the mean and standard deviation manually\naverage <- sum(x)/length(x)\nSD <- sqrt(sum((x-average)^2)/length(x))\n\n# built-in mean and sd functions - note that the audio and printed value disagree\naverage <- mean(x)\nSD <- sd(x)\nc(average = average, SD = SD)\n\n# calculate standard units\nz <- scale(x)\n\n# calculate proportion of value within 2 SD of mean\nmean(abs(z) < 2)\n\n\nfunction sd():The built-in R function sd() calculates the standard deviation, but it divides by length(x)-1 instead of length(x). When the length of the list is large, this difference is negligible and you can use the built-in sd() function. Otherwise, you should compute σ by hand. For this course series, assume that you should use the sd() function unless you are told not to do so.\nHere we will learn more about benchmark z-score value and their corresponding probabilities.\n\n\n2.8.2 The 68-95-99.7 Rule\nThe normal distribution is associated with the 68-95-99.7 rule. This rule describes the probability of observing events within a ceration number of standard deviations of the mean.\n\n\n\nNormal Distribution Probabilities\n\n\nThe probability distribution function for the normal distribution is defined such that:\n\nAbout 68% of observations will be within one standard deviation of the mean(\\mu\\pm\\sigma). In standard units, this is equivalent to a z-score of |z|\\leq2\n\n\n\n\nProbability of an observation within 1 SD of mean\n\n\n\nAbout 95% of observations will be within two standard seviations of the mean(\\mu\\pm2\\sigma). In standard units, this is equivalent to a z-sore of |z|\\leq2.\n\n\n\n\nProbability of an ovservation within 2 SD of mean\n\n\n\nAbout 99.7% of observations will be within three standard deviations of the mean(\\mu\\pm3\\sigma). In standard units, this is equivalent to a z-score of |z|\\leq3.\n\n\n\n\nProbability of an observation within 3 SD of mean"
  },
  {
    "objectID": "Datavisualization.html#the-normal-cdf-and-pnorm",
    "href": "Datavisualization.html#the-normal-cdf-and-pnorm",
    "title": "Data Science: Data Visualization",
    "section": "2.9 The Normal CDF and pnorm",
    "text": "2.9 The Normal CDF and pnorm\n\n2.9.1 Key points:\n\nThe normal distribution has a mathematically defined CDF which can be computed in R with the function pnorm.\npnom(a, avg, s) gives the value of the cumculative distribution function F(a) for the normal distribution defined by average avg and standard deviation s.\nwe say that a random quantity is normally distributed with average avg and standard deviation s if the approximate pnorm(a, avg, s) holds for all values of a.\nIf we are willing to use the normal approximation for height, we can estimate the distribution simply from the mean and standard deviation of our values.\nIf we treat the height data as discrete rather than categorical, we see that the data are not very useful because integer values are more common that expected due to rounding. This is called discretization.\nWith rounded data, the normal approximation is particularly useful when computing probabilities of intervals of length 1 that include exactly over integer.\n\n\n\n2.9.2 Code: Using pnorm to calculate probabilities\nGiven male heights x:\n\n\nshow the code\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(\"heights\")\nx <- heights %>% filter(sex==\"Male\") %>% pull(height)\n\n\nwe can estimate the probability that a male is taller than 70.5 inches with:\n\n\nshow the code\n1 - pnorm(70.5, mean(x), sd(x))\n\n\n\n\n2.9.3 Code: Discretization and the normal approximation\n\n\nshow the code\n# plot distribution of exact heights in data\nplot(prop.table(table(x)), xlab = \"a = Height in inches\", ylab = \"Pr(x = a)\")\n\n\n\n\n\n\n\nshow the code\n# probabilities in actual data over length 1 ranges containing a integer\nmean(x <= 68.5) - mean(x <= 67.5)\nmean(x <= 69.5) - mean(x <= 68.5)\nmean(x <= 70.5) - mean(x <= 69.5)\n\n# probabilities in normal approximation match well\npnorm(68.5, mean(x), sd(x)) - pnorm(67.5, mean(x), sd(x))\npnorm(69.5, mean(x), sd(x)) - pnorm(68.5, mean(x), sd(x))\npnorm(70.5, mean(x), sd(x)) - pnorm(69.5, mean(x), sd(x))\n\n# probabilities in actual data over other ranges don't match normal approx as well\nmean(x <= 70.9) - mean(x <= 70.1)\npnorm(70.9, mean(x), sd(x)) - pnorm(70.1, mean(x), sd(x))"
  },
  {
    "objectID": "Datavisualization.html#definition-of-quantiles",
    "href": "Datavisualization.html#definition-of-quantiles",
    "title": "Data Science: Data Visualization",
    "section": "2.10 Definition of quantiles",
    "text": "2.10 Definition of quantiles\n\n2.10.1 Definition of quantiles\nQuantiles are cut off points that divide a dataset into intervals with set probability. The qth quantile is the value at which q% of the observation are equal to or less than that value.\n\n\n2.10.2 Using the quantile function\nGiven a dataset data and desired quantile q, you can find the q the quantile of data with:\n\n\nshow the code\nquantile(data,q)\n\n\n\n\n2.10.3 Percentiles\nPercentiles are the quantiles that divide a dataset into 100 intervals each with 1% probability. You can determine all percentiles of a dataset data like this:\n\n\nshow the code\np <- seq(0.01, 0.09, 0.01)\nquantile(data, p)\n\n\n\n\n2.10.4 Quartiles\nQuartiles divide a dataset into 4 parts each with 25% probability. They are equal to the 25th, 50th and 75th percentiles. The 25th percentile is also known as the 1st quartile, the 50th percentile is also konwn as the median, and the 75th percentile is also knowns as the 3rd quartile.\nThe summary() function returns the minimum, quartiles and maximum of a vector.\n\n\n2.10.5 Examples\nLoad the heights dataset from the dslabs package:\n\n\nshow the code\nlibrary(dslabs)\ndata(\"heights\")\n\n\nUsesummaryon the heights$height variable to find the quartiles:\n\n\nshow the code\nsummary(heights$height)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  50.00   66.00   68.50   68.32   71.00   82.68 \n\n\nFind the percentiles of height$height:\n\n\nshow the code\np <- seq(0.01, 0.99, 0.01)\npercentiles <- quantile(heights$height, p)\n\n\nConfirm that the 25th and 75th percentiles match that 1st and 3rd quartiles. Note that quantile() returns a named vector. You can access the 25th and 75th percentiles like this (adapt the code for other percentile value):\n\n\nshow the code\npercentiles[names(percentiles) == \"25%\"]\n\n\n25% \n 66 \n\n\nshow the code\npercentiles[names(percentiles) == \"75%\"]\n\n\n75% \n 71"
  },
  {
    "objectID": "Datavisualization.html#finding-quantile-with-qnorm",
    "href": "Datavisualization.html#finding-quantile-with-qnorm",
    "title": "Data Science: Data Visualization",
    "section": "2.11 Finding quantile with qnorm",
    "text": "2.11 Finding quantile with qnorm\n\n2.11.1 Definiton of qnorm\n简单来说,qnorm是正态分布累积分布函数(CDF)的反函数， 也就是说它可以视为pnorm的反函数, 这里q指的是quantile, 即分位数\nThe qnorm() function gives the theoretical value of a quantile with probability p of observing a value equal to or less than that quantile value a normal distribution with mean mu and standard deviation sigma:\n\n\nshow the code\nqnorm(p, mu, sigma)\n\n\nBy default, mu=0 and sigma=1. Therefore, calling qnorm() with no arguments gives quantiles for the standard normal distribution.\n\n\nshow the code\nqnorm(p)\n\n\nRecall that quantiles are defined such that p is the probability of a random observation less than or equal to the quantile.\n\n\n2.11.2 Realation to pnorm\nThe pnorm() function gives the probability that a value from a standard normal distribution will be less than or equal to a z-score value z. consider: pnorm(-1.96)\\approx0.025 The result of pnorm() is the quantile. Note that: qnorm(0.025)\\approx-1.96 qnorm() and pnorm are inverse functions: pnorm(qnorm(0.025))\\equiv0.025\n\n\n2.11.3 Theoretical quantiles\nYou can use qnorm() to determine the theoretical quantiles of a dataset: that is, the theoretical value of quantiles assuming that a dataset follows a normal distribution. Run the qnorm() function with the desired probabilities p, mean mu and standard deviation sigma.\nSuppose male heights follow a normal distribution with a mean of 69 inches and standard deviation of 3 inches. The theoretical quantiles are:\n\n\nshow the code\np <- seq(0.01, 0.99, 0.01)\ntheoretical_quantiles <- qnorm(p, 69, 3)\n\n\nTheoretical quantiles can be compared to sample quantiles determined with the quantile function in order to evaluate whether the sample follows a normal distribution."
  },
  {
    "objectID": "Datavisualization.html#quantile-quantile-plots",
    "href": "Datavisualization.html#quantile-quantile-plots",
    "title": "Data Science: Data Visualization",
    "section": "2.12 Quantile-Quantile Plots",
    "text": "2.12 Quantile-Quantile Plots\n\n2.12.1 Key Points:\n\nQuantile-quantile plots, or QQ-plot, are used to check whether distributions are well-approximated by a normal distribution.\nGiven a proportion p, the quantile q is the value such that the proportion of values in the data blew q is p.\nIn a QQ-plot, the sample quantiles in the observed data are compared to the theoretical quantiles expected from the normal distribution. If the data are well-approximated by the normal distribution, then the points on the QQ-plot will fall near the identity line(sample = theoretical).\nCalculate sample quantiles (observed quantiles) using the quantile() function.\nCalculate theoretical quantiles with the qnorm() function. qnorm() will caculate quantiles for the standard normal distribution (\\mu=0, \\sigma=1) by default, but it can calculate quantiles for any normal distribution given mean() and sd() arguments.\n\n\n\n2.12.2 Code:\n\n\nshow the code\n# define x and z\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(heights)\n\nindex <- heights$sex==\"Male\"\nx <- heights$height[index]\nz <- scale(x)\n\n# proportion of data below 69.5\nmean(x <= 69.5)\n\n\n[1] 0.5147783\n\n\nshow the code\n# calculate observed and theoretical quantiles\np <- seq(0.05, 0.95, 0.05)\nobserved_quantiles <- quantile(x, p)\ntheoretical_quantiles <- qnorm(p, mean = mean(x), sd = sd(x))\n\n# make QQ-plot\nplot(theoretical_quantiles, observed_quantiles)\nabline(0,1)\n\n\n\n\n\nshow the code\n# make QQ-plot with scaled values\nobserved_quantiles <- quantile(z, p)\ntheoretical_quantiles <- qnorm(p)\nplot(theoretical_quantiles, observed_quantiles)\nabline(0,1)"
  },
  {
    "objectID": "Datavisualization.html#percentiles-1",
    "href": "Datavisualization.html#percentiles-1",
    "title": "Data Science: Data Visualization",
    "section": "2.13 Percentiles",
    "text": "2.13 Percentiles\n\n2.13.1 Key Points:\n\nPercentiles are the quantiles obtained when defining p as 0.01, 0.02,…,0.99. They summarize the values at which a certain percent of the observations are equal to or less than that value.\nThe 50th percentile is also known as the median.\nThe quartiles are the 25th, 50th and 75th percentiles."
  },
  {
    "objectID": "Datavisualization.html#boxplots",
    "href": "Datavisualization.html#boxplots",
    "title": "Data Science: Data Visualization",
    "section": "2.14 Boxplots",
    "text": "2.14 Boxplots\nR语言如何绘制箱线图\n\n2.14.1 Key Points:\n\nWhen data do not follow a normal distribution and cannot be succinctly summarized by only the mean and standard deviation, an alternative is to report a five-number summary: range (ignoring outliers) and the quartiles (25th, 50th, 75th percentile).\nIn a boxplot, the box is defined by the 25th and 75th percentiles and the median is a horizontal line through the box. The whiskers show the range excluding outliers, and outliers are plotted separately as individual points.\nThe interquartile range is the distance between the 25th and 75th percentiles.\nBoxplots are particularly useful when comparing multiple distributions."
  },
  {
    "objectID": "Datavisualization.html#distribution-of-female-heights",
    "href": "Datavisualization.html#distribution-of-female-heights",
    "title": "Data Science: Data Visualization",
    "section": "2.15 Distribution of Female Heights",
    "text": "2.15 Distribution of Female Heights\n\n2.15.1 Key Points:\n\nIf a distribution is not normal, it cannot be summarized with only the mean and standard seviation. Provide a histogram, smooth density or boxplot instead.\nA plot can force us to see unexpected results that make us question the quality or implication of our data."
  },
  {
    "objectID": "Datavisualization.html#overview-1",
    "href": "Datavisualization.html#overview-1",
    "title": "Data Science: Data Visualization",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nAfter completing ggplot2, we will:\n\nbe able to use ggplot2 to create data visualizations in R.\nbe able to explain what the data component of a graph is.\nbe able to identify the geometry component of a graph and know when to use which type of geometry. be able to explain what the aesthetic mapping component of a graph is.\nbe able to understand the scale component of a graph and select an appropriate scale component to use."
  },
  {
    "objectID": "Datavisualization.html#ggplot",
    "href": "Datavisualization.html#ggplot",
    "title": "Data Science: Data Visualization",
    "section": "3.2 ggplot",
    "text": "3.2 ggplot\n\n3.2.1 ggplot2\n\nData visualization with ggolot2\nData visualization with ggplot2: Cheat Sheet\nThe R graph gallery example\n\n\n\n3.2.2 key Points:\n\nThroughout the series, we will create plots with the ggplot2 package. ggplot2 is part of the tidyverse suite of package, which you can load with library(tidyverse).\nNote that you can also load ggplot2 alone using the command library(ggplot2), instead of loading the entire tidyverse.\nggplot2 uses a grammar of graphics to break plots into building blocks that have intuitive syntax, making it easy to create relatively complex and aesthetically pleasing plots with relatively simple and readable code.\nggplot2 is designed to work excusively with tidy data (rows are observations and columns are variables)."
  },
  {
    "objectID": "Datavisualization.html#graph-components",
    "href": "Datavisualization.html#graph-components",
    "title": "Data Science: Data Visualization",
    "section": "3.3 Graph Components",
    "text": "3.3 Graph Components\n\n3.3.1 Key Points:\n\nPlots in ggplot2 consist of 3 main components:\n\nData: The dataset being summarized\nGeometry: The type of plot(scatterplot, boxplot, barplot, histogram, qqplot, smooth desity, etc.)\nAesthetic mapping: Variable mapped to visual cues, such as x-axis and y-axis values and color.\n\n\n\n\n3.3.2 Code:\n\n\nshow the code\nlibrary(dslabs)\ndata(murders)"
  },
  {
    "objectID": "Datavisualization.html#creating-a-new-plot",
    "href": "Datavisualization.html#creating-a-new-plot",
    "title": "Data Science: Data Visualization",
    "section": "3.4 Creating a New Plot",
    "text": "3.4 Creating a New Plot\n\n3.4.1 Key Points:\n\nYou can associated a dataset x with a ggplot object with any of the 3 commands:\n\nggplot(data = x)\nggplot(x)\nx %>% ggplot()\n\nYou can assign a ggplot object to a variable. If the object is not assigned to a variable, it will automatically be displayed.\nYou can display a ggplot object assigned to a variable by printing that variable.\n\nCode:\n\n\nshow the code\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(murders)\n\nggplot(data = murders)\n\nmurders %>% ggplot()\n\np <- ggplot(data = murders)\n\nclass(p)\n\nprint(p) # this is equivalent to simply typing p\np"
  },
  {
    "objectID": "Datavisualization.html#layers",
    "href": "Datavisualization.html#layers",
    "title": "Data Science: Data Visualization",
    "section": "3.5 Layers",
    "text": "3.5 Layers\n\n3.5.1 Key Points:\n\nIn ggplot2, graphs are created by adding layers to the ggplot object: DATA %>% ggplot() + LAYER_1 + LAYER_2 + … + LAYER_N\nThe geometry layer defines that plot type and takes the format geom_x where x is the plot type.\nAesthetic mappings describe how properties of the data connect with features of the graph (axis position, color, size, etc.) define aesthetic mapping with aes() function.\naes() uses variable names from the object component (for example, total rather than murders$total).\ngeom_point() creates a scatterplot and requires x and y aesthetic mappings.\ngeom_text() and geom_label add text to a scatterplot and require x, y, and label aesthetic mappings.\nTo determine which aesthetic mappings are required for a geometry, read the help file for that geometry.\nYou can add layers with different aesthetic mappings to the same graph.\n\nCode: Adding layers to a plot\n\n\nshow the code\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(murders)\n\nmurders %>% ggplot() +\n  geom_point(aes(x = population/10^6, y = total))\n\n\n\n\nshow the code\n# add points layer to predefined ggplot object\np <- ggplot(data = murders)\np + geom_point(aes(population/10^6, total))\n\n\n\n\n\nshow the code\n# add text layer to scatterplot\np + geom_point(aes(population/10^6, total)) +\n  geom_text(aes(population/10^6, total, label = abb))\n\n\n\n\n\nCode: Example of aes behavior\n\n\nshow the code\n# no error from this call\np_test <- p + geom_text(aes(population/10^6, total, lable = abb))\n\n# error - \"abb\" is not a globally defined variable and cannot be found outside of aes\np_test <- p + geom_text(aes(population/10^6, total), label = abb)"
  },
  {
    "objectID": "Datavisualization.html#thinkering",
    "href": "Datavisualization.html#thinkering",
    "title": "Data Science: Data Visualization",
    "section": "3.6 Thinkering",
    "text": "3.6 Thinkering\n\n3.6.1 Key Points:\n\nYou can modify arguments to geometry functions others than aes() and the data.\nThese arguments are not aesthetic mappings: the affect all data points the same way.\nGlobal aesthetic mappings apply to all geometries and can be defined when you initially call ggplot(). All the geometries added as layers will default to this mapping. Local aesthetic mapping add additional information or override the default mappings.\n\n\n\n\n\n\n\nNudge points a fixed distance\n\n\n\nposition_nudge(x = 0, y = 0) is generally useful for adjusting the position of items on discrete scales by a small amount. Nudging is built in to geom_text() because it’s so useful for moving labels a small distance from what they’re labeling.\n\n\nCode:\n\n\nshow the code\n# change the size of the points\np + geom_point(aes(population/10^6, total), size = 3) +\n    geom_text(aes(population/10^6, total, label = abb))\n\n\n\n\n\nshow the code\n# move text labels slightly to the right\np + geom_point(aes(population/10^6, total), size = 3) +\n    geom_text(aes(population/10^6, total, label = abb), nudge_x = 1)\n\n\n\n\n\nshow the code\n# simplify code by adding global aesthetic\np <- murders %>% ggplot(aes(population/10^6, total, label = abb))\np + geom_point(size = 3) +\n    geom_text(nudge_x = 1.5)\n\n\n\n\n\nshow the code\n# local aesthetics override global aesthetics\np + geom_point(size = 3) +\n  geom_text(aes(x = 10, y = 800, label = \"Hello there!\"))"
  },
  {
    "objectID": "Datavisualization.html#scales-labels-and-colors",
    "href": "Datavisualization.html#scales-labels-and-colors",
    "title": "Data Science: Data Visualization",
    "section": "3.7 Scales, Labels, and Colors",
    "text": "3.7 Scales, Labels, and Colors\n\n3.7.1 Textbook links:\n\nTextbook section on scales\nTextbook section on labels and titles\nTextbook section on categories as colors\nTextbook section on annotation, shapes and adjustments\n\n\n\n3.7.2 Key Points:\n\nConvert the x-axis to log scale with scale_x_continuous(trans = \"log10\") or scale_x_log10(). Similar function exist for the y-axis.\nAdd axis title with xlab() and ylab() function. Add a plot title with the ggtitle() function.\nAdd a color mapping that colors points by a varaibale by defining col argument within aes(). To color all pints the same way, define col outside of aes().\nAdd a line with the geom_abline() geometry. geom_abline() takes arguments slop (default = 1) and intercept(default = 0). Change the color with col or color and line type with lty.\nPlacing the line layer after the point layer will overlay the the line on top of the points. To overlay points on the line, place the line layer before the point layer.\nThere are many additional ways to tweak your graph that can be found in the ggplot2 documentation, cheat sheet or on the internet. For example, you can change the legend title with scale_color_discrete.\n\n\n\n3.7.3 Code: Log-scale the x-axis and y-axis\n\n\nshow the code\n# define p\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(murders)\n\np <- murders %>% ggplot(aes(population/10^6, total, label = abb))\n\n# log base 10 scale the x-axis and y-axis\np + geom_point(size = 3) +\n    geom_text(nudge_x = 0.05) +\n    scale_x_continuous(trans = \"log10\") +\n    scale_y_continuous(trans = \"log10\")\n\n\n\n\nshow the code\n# efficient log scaling of the axes\np + geom_point(size = 3) +\n    geom_text(nudge_x = 0.05) +\n    scale_x_log10() +\n    scale_y_log10()\n\n\n\n\n\n\n\n3.7.4 Code: Add labels and title\n\n\nshow the code\np + geom_point(size = 3) +\n    geom_text(nudge_x = 0.05) +\n    scale_x_log10() +\n    scale_y_log10() +\n    xlab(\"Population in million(log scale)\") +\n    ylab(\"Total number of murders(log scale)\") +\n    ggtitle(\"US Gun Murders in 2010\")\n\n\n\n\n\n\n\n3.7.5 Code: Change color of the points\n\n\nshow the code\n# redefine p to be everything except the points layer\np <- murders %>% \n     ggplot(aes(population/10^6, total, label = abb)) +\n     geom_text(nudge_x = 0.075) +\n     scale_x_log10() +\n     scale_y_log10() +\n     xlab(\"Population in million(log scale)\") +\n     ylab(\"Total number of murders(log scale)\") +\n     ggtitle(\"US Gun Murders in 2010\")\n\n\n\n\nshow the code\n# make all points blue\np + geom_point(size = 3, color = \"blue\")\n\n\n\n\n\n\n\nshow the code\n# color points by region\np + geom_point(aes(col = region), size = 3)\n\n\n\n\n\n\n\n3.7.6 Code: Add a line with average murder rate\n\n\nshow the code\nr <- murders %>% \n     summarize(rate = sum(total) / sum(population) * 10^6) %>%      pull(rate)\n\np <- p + geom_point(aes(col = region), size = 3) +\n         geom_abline(intercept = log10(r)) # slop is default of 1\n\n# change line to dashed and dark grey, line under points\np + geom_abline(intercept = log(r), lty = 2, color = \"darkgrey\") +\n    geom_point(aes(col = region), size = 3)\n\n\n\n\n\n\n\n3.7.7 Code: Change legend title\n\n\nshow the code\n# capitalize legend title\np <- p + scale_color_discrete(name = \"Region\")\np"
  },
  {
    "objectID": "Blog.html",
    "href": "Blog.html",
    "title": "Stats & R",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDynamic Graph\n\n\n0 min\n\n\n\ncode\n\n\nanalysis\n\n\n\nMake a dynamic graph by gganmate\n\n\n\nNing Li\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChristmas Tree\n\n\n0 min\n\n\n\ncode\n\n\n\nMerry Christmas Everyone!!!\n\n\n\nNing Li\n\n\nNov 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome\n\n\n0 min\n\n\n\nnews\n\n\n\nWelcome !!!\n\n\n\nNing Li\n\n\nNov 27, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  }
]